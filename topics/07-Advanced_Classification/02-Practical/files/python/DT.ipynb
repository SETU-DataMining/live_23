{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bc0fc0a9",
      "metadata": {},
      "source": [
        "# Using Decision Trees: detailed example\n",
        "\n",
        "The _Play Tennis_ example data (from Mitchell (1997) _Machine Learning_) used in the notes for Week 8 is a nice, relatively small example.\n",
        "\n",
        "The predictors are discrete-valued labels, making the decision splits easier to understand.\n",
        "\n",
        "The first step is to read the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a1e91b6",
      "metadata": {
        "tags": [
          "load"
        ]
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "import pprint\n",
        "pp = pprint.PrettyPrinter(indent=2)\n",
        "dataDir = \"data\"\n",
        "df = pd.read_csv(dataDir+\"/playTennis.csv\",header=0,quotechar=\"'\")\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f928415f",
      "metadata": {
        "tags": [
          "intro"
        ]
      },
      "source": [
        "For convenience, I have created a function to evaluate the _entropy_ function `H`.\n",
        "\n",
        "It looks more complex than it is - mainly because I assemble the calculation in a string, for teaching purposes. This string is returned to the caller and `eval`uated there by the calling program.\n",
        "\n",
        "Note that `H` has two forms, depending on whether it is Level 1 (applies to the overall set: arguments `predNameB` and `predLabelB` are each empty strings and `pd` evaluates to the number of rows in the overall set) or Level 2 (when it is conditional on a setting of one of the predictors, specified by `predNameB` and `predLabelB` and `pd` is also more specific).\n",
        "\n",
        "If a particular predictor setting does not arise for a given class setting, the row count `pn` is zero and the log term needs special treatment. The log of zero is minus infinity, but the log term itself is multipled by zero, so the two multiplied together is taken as zero.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43f60d0e",
      "metadata": {
        "tags": [
          "calcH"
        ]
      },
      "outputs": [],
      "source": [
        "def calcH(predNameB,predLabelB,pd):\n",
        "  acc = \"\"\n",
        "  for classLabel in labels[className]:\n",
        "    pns = \"rowCount[className]\"+predNameB+\"[classLabel]\"+predLabelB\n",
        "    pn = eval(pns)\n",
        "    p = \"(\"+str(pn) +\"/\" + str(pd)+\")\"\n",
        "    if (pn == 0):\n",
        "      acc += \" -\" + p + \" \"\n",
        "    else:\n",
        "      acc += \" -\" + p + \" * math.log(\" + p + \", 2) \"\n",
        "  return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "181b5788",
      "metadata": {
        "tags": [
          "countsByAttribute"
        ]
      },
      "source": [
        "## Preliminary: Deriving the row counts\n",
        "\n",
        "We now start to count rows depending on various (combinations of) settings. We use a rowCount set to store the counts. The first dictionary element gets the count of `'ALL'` rows.\n",
        "\n",
        "We then get a list of the column names `colNames`and the names of the class to be predicted `className`.\n",
        "\n",
        "Looping over the column names, we ask what the unique list of labels is for each column name.\n",
        "\n",
        "We can then loop over each of these labels and count the number of rows for which this column name takes a particular label value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26bd45dc",
      "metadata": {
        "tags": [
          "computeCounts"
        ]
      },
      "outputs": [],
      "source": [
        "rowCount = {}\n",
        "rowCount[\"ALL\"] = len(df.index)\n",
        "colNames = list(df.columns.values)\n",
        "className = \"play\"\n",
        "labels = {}\n",
        "for colName in colNames:\n",
        "  labels[colName] = df[colName].unique().tolist()\n",
        "  rowCount[colName] = {}\n",
        "  for label in labels[colName]:\n",
        "    rowCount[colName][label] = len(df.loc[df[colName] == label].index)\n",
        "pp.pprint(rowCount)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe49d659",
      "metadata": {
        "tags": [
          "level2"
        ]
      },
      "source": [
        "For the Decision Tree classifier, we need to go to Level 2 also.\n",
        "\n",
        "Therefore we also need to split the predictor counts above based on whether the decision was to play or not.\n",
        "\n",
        "Our first step is to use a _list comprehension_ to return a list of the predictor column names only `predNames`.\n",
        "\n",
        "We can then loop over just the predictors.\n",
        "\n",
        "The resulting `rowCount` values depend on both the `className` = `classLabel` and the `predName` = `predLabel` conditions.\n",
        "\n",
        "We then print the rowCount variable and see that `rowCount` has been extended with these counts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b21d36d7",
      "metadata": {
        "lines_to_next_cell": 2,
        "tags": [
          "calcLevel2"
        ]
      },
      "outputs": [],
      "source": [
        "predNames = [x for x in colNames if className not in x]\n",
        "for predName in predNames:\n",
        "  rowCount[className][predName] = {}\n",
        "  for classLabel in labels[className]:\n",
        "    rowCount[className][predName][classLabel] = {}\n",
        "    for predLabel in labels[predName]:\n",
        "      rowCount[className][predName][classLabel][predLabel] = len(df.loc[(df[className] == classLabel) & (df[predName] == predLabel)].index)\n",
        "pp.pprint(rowCount)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "685bc3ff",
      "metadata": {
        "tags": [
          "entropy"
        ]
      },
      "source": [
        "## Entropy calculations\n",
        "\n",
        "The top-level entropy calculation is calculated without any splits by predictor.\n",
        "\n",
        "It is calculated in terms of the class column (\"play\" in this case) which takes two values: \"yes\" and \"no\". As expected it takes a value based on how predictable this variable is.\n",
        "\n",
        "Note that the `calcH` function is called with empty strings for `predNameB`  and `predLabelB` and the `rowCount` for ALL settings.\n",
        "\n",
        "Also, the string returned by `calcH` needs to be evaluated to a number.\n",
        "\n",
        "We print both the `H` expression (as a string) and its value (as a number)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c19c004",
      "metadata": {
        "tags": [
          "calcEntropy"
        ]
      },
      "outputs": [],
      "source": [
        "H = {}\n",
        "H[className] = {}\n",
        "acc = calcH(predNameB=\"\",predLabelB=\"\",pd=rowCount[\"ALL\"])\n",
        "H[className][\"\"] = eval(acc)\n",
        "\n",
        "print(acc)\n",
        "print(H[className][\"\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26e9371f",
      "metadata": {
        "tags": [
          "split"
        ]
      },
      "source": [
        "We now consider the effects of splitting by each of the predictors in turn.\n",
        "\n",
        "In each case we scale by `p` which is the probability of a particular setting.\n",
        "\n",
        "The other term is of course the `H` for that setting with the class variable. Note that here `calcH` has `predNameB` and `predLabelB` that are not just empty strings.\n",
        "\n",
        "Using the `term` counter, we can tell whether we need to start a new accumulated string or just add to an existing accumulated string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b2a9f9b",
      "metadata": {
        "lines_to_next_cell": 2,
        "tags": [
          "calcSplit"
        ]
      },
      "outputs": [],
      "source": [
        "for predName in predNames:\n",
        "  term = 0\n",
        "  predNameB = '[\"'+predName+'\"]'\n",
        "  for predLabel in labels[predName]:\n",
        "    predLabelB = '[\"'+predLabel+'\"]'\n",
        "    p = \"(\" + str(rowCount[predName][predLabel]) + \"/\" + str(rowCount[\"ALL\"]) + \") * \"\n",
        "    a = p + \"(\"+calcH(predNameB,predLabelB,pd=rowCount[predName][predLabel])+\")\"\n",
        "    if (term == 0):\n",
        "      acc = a\n",
        "    else:\n",
        "      acc = acc + \" + \" + a\n",
        "    term += 1\n",
        "  H[className][predName] = eval(acc)\n",
        "\n",
        "  print(acc)\n",
        "  print(\"Splitting on {} gives entropy {}\".format(predName,H[className][predName]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c540af0",
      "metadata": {
        "tags": [
          "choice"
        ]
      },
      "source": [
        "So which predictor should we use in our node, to split the data to get the smallest entropy over the available predictors?\n",
        "\n",
        "It seems that the `outlook` variable is the one to choose!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc8f0946",
      "metadata": {
        "lines_to_next_cell": 2,
        "tags": [
          "calcChoice"
        ]
      },
      "outputs": [],
      "source": [
        "splitVariable = min(H[className], key=H[className].get)\n",
        "print(\"The first split should be on the _{}_ variable, reducing the entropy from {} to {}\".format(splitVariable,H[className][\"\"],H[className][splitVariable]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35f41f31",
      "metadata": {
        "tags": [
          "discussionManual"
        ]
      },
      "source": [
        "Note that this process can be continued as necessary, with it stopping when all leafs are _pure_ (have entropy = 0). According to https://codefying.com/2015/03/09/decision-tree-classifier-part-1/, the resulting decision tree is shown below:\n",
        "\n",
        "![View of Play Tennis decision tree](https://codefying.files.wordpress.com/2015/03/mltree.jpg)\n",
        "\n",
        "Note that the `temp` predictor is not used in the decision tree - it is not needed."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4d7e083",
      "metadata": {
        "tags": [
          "discussionSklearn"
        ]
      },
      "source": [
        "## Using sklearn to derive the decision tree\n",
        "\n",
        "Note that the treatment above is intended to help understanding. There is no need to program Decision Tree classifiers yourself!\n",
        "\n",
        "`scikit-learn` offers a `DecisionTreeClassifier` with the same sort of API as other supervised learning algorithms, alongside settings that are specific to Decision Trees.\n",
        "\n",
        "One awkward feature of the sklearn implementation is that it seems to be necessary to code the labels as integers. While there are tools to do this, it does mean that the resulting decision tree is not as easy to \"read\""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
