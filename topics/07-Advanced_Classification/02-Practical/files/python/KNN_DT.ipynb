{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cee35eba",
      "metadata": {},
      "source": [
        "# Classifying iris plants according to their measurements\n",
        "\n",
        "We use sklearn to classify the iris data we saw back in Week 1.\n",
        "\n",
        "The data is included in scikit-learn, so we can get it directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb17d084",
      "metadata": {
        "tags": [
          "load"
        ]
      },
      "outputs": [],
      "source": [
        "from sklearn import neighbors, datasets\n",
        "import pandas as pd\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "print(dir(iris)) # Note that it is not a dataframe, more a generalised data object\n",
        "print(iris.feature_names)\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# create the model\n",
        "knn = neighbors.KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# fit the model\n",
        "knn.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "536a14e9",
      "metadata": {},
      "source": [
        "With knn, you can determine membership probabilities for each of the 3 labels. As you can see, the predict() function just picks the most likely label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89d94cc4",
      "metadata": {
        "tags": [
          "knn_predict"
        ]
      },
      "outputs": [],
      "source": [
        "# What kind of iris has 3cm x 5cm sepal and 4cm x 2cm petal?\n",
        "# call the \"predict\" method:\n",
        "result = knn.predict([[3, 5, 4, 2],])\n",
        "\n",
        "print(iris.target_names[result])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ab1fbbc",
      "metadata": {
        "tags": [
          "knn_predictProb"
        ]
      },
      "outputs": [],
      "source": [
        "knn.predict_proba([[3, 5, 4, 2],]) "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69144f02",
      "metadata": {},
      "source": [
        "Make sure the directory exists beforehand to store the generated plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "279d7f93",
      "metadata": {
        "tags": [
          "ensureOutput"
        ]
      },
      "outputs": [],
      "source": [
        "import os\n",
        "picDir = \"output/pics\"\n",
        "if not os.path.exists(picDir):\n",
        "    os.makedirs(picDir)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aca077cd",
      "metadata": {},
      "source": [
        "Remember: the label (0,1,2) maps to setosa, versicolor and virginica. Therefore, the probability that it is setosa, versicolor or virginica is 0, 0.8 and 0.3, respectively. Clearly versicolor is chosen!\n",
        "\n",
        "In the next block of code, we take each pair of predictors from the four available in the Iris data set, and use the k-nearest-neighbour algorithm with k=3,5,7. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cc2a41d",
      "metadata": {
        "lines_to_next_cell": 2,
        "tags": [
          "fitsWithPlots"
        ]
      },
      "outputs": [],
      "source": [
        "import pylab as pl\n",
        "import numpy as np\n",
        "from matplotlib.colors import ListedColormap\n",
        "import itertools\n",
        "import re, string\n",
        "from plotSupp import plot_2d_class\n",
        "\n",
        "# Create color maps for 3-class classification problem, as with iris\n",
        "cmap_light = ListedColormap(['#FFDDDD', '#DDFFDD', '#DDDDFF'])\n",
        "cmap_bold = ListedColormap(['#FF2222', '#22FF22', '#8888FF'])\n",
        "\n",
        "#predNames = list(iris.data) # https://stackoverflow.com/a/19483025, except iris.data is an array, not a dataframe\n",
        "predNames = iris.feature_names\n",
        "df=pd.DataFrame(iris.data, columns=predNames)\n",
        "nTrain = df.shape[0]\n",
        "y = iris.target\n",
        "pattern = re.compile('[\\W_]+', re.UNICODE) # https://stackoverflow.com/a/1277047\n",
        "for neighborCnt in range(3,8,2): # from 3 to a maximum of 8, in steps of 2, so 3,5,7\n",
        "  knn = neighbors.KNeighborsClassifier(n_neighbors=neighborCnt)\n",
        "  for twoCols in itertools.combinations(predNames, 2): # https://stackoverflow.com/a/374645\n",
        "    X = df[list(twoCols)]  # we only take two features at a time\n",
        "    colNames = X.columns\n",
        "    c1 = colNames[:1][0] # first of 2\n",
        "    c2 = colNames[-1:][0] # last of 2\n",
        "    c1 = pattern.sub(\"\",c1.title()) # Make titlecase, then remove non-alphanumeric characters\n",
        "    c2 = pattern.sub(\"\",c2.title())\n",
        "    knn.fit(X, y)\n",
        "    plotTitle = \"k = %i %s fit to the %s dataset\" % (neighborCnt, \"nearest-neighbours\", \"Iris\")\n",
        "    fileTitle = picDir + \"/k_%i_%s_%s_%s_%s.pdf\" % (neighborCnt, \"nearest-neighbours\", \"Iris\", c1, c2)\n",
        "    print(\"Plotting file %s\" % (fileTitle))\n",
        "    plot_2d_class(X, y, nTrain, knn, plotTitle, fileTitle, cmap_light, cmap_bold)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2df2fcda",
      "metadata": {},
      "source": [
        "## Model Validation\n",
        "\n",
        "The k-nearest-neighbours classification \"model\" should be validated. Clearly, the parameter $k$ is critical to its performance. Generally, smaller values of $k$ fit the training set more accurately (less bias) but generalise less well to test data (more variance). The opposite applies to larger values of $k$.\n",
        "\n",
        "With $k$ set to its minimum value ($k = 1$), it fits the training set exactly and the confusion matrix is optimal:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00a40caa",
      "metadata": {
        "tags": [
          "kequals1"
        ]
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "X, y = iris.data, iris.target\n",
        "knn1 = KNeighborsClassifier(n_neighbors=1)\n",
        "knn1.fit(X, y)\n",
        "y_pred1 = knn1.predict(X)\n",
        "print(np.all(y == y_pred1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9da63cd",
      "metadata": {},
      "source": [
        "The *confusion matrix* highlights where classification differences arise, as these occur on the off-diagognal elements of the matrix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bec42333",
      "metadata": {
        "tags": [
          "kequals1metrics"
        ]
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "print(accuracy_score(y, y_pred1))\n",
        "print(confusion_matrix(y, y_pred1))\n",
        "print(classification_report(y, y_pred1, digits=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edb111f5",
      "metadata": {
        "tags": [
          "metricsDiscussion"
        ]
      },
      "source": [
        "All 50 training samples for each class are identified correctly, as expected when $k = 1$ (accuracy score is 1, off-diagonal terms are 0, the classification report (relative to the trsining set) is \"too good to be true\"...\n",
        "\n",
        "Note:\n",
        "\n",
        "1. The _Recall_ of the $i^{\\mbox{th}}$ predictor is $R_i \\equiv c_{ii} / \\sum_j c_{ij}$, which is the ratio of the $i^{\\mbox{th}}$ diagonal element to the sum of the elements of the confusion matrix $C = \\{c_{ij}\\}$ in that _column_.\n",
        "2. The _Precision_ of the $j^{\\mbox{th}}$ predictor is $P_j \\equiv c_{jj} / \\sum_i c_{ij}$, which is the ratio of the $j^{\\mbox{th}}$ diagonal element to the sum of the elements of the confusion matrix $C = \\{c_{ij}\\}$ in that _row_.\n",
        "3. $F_1$-score is defined as $F_1 = 2\\frac{R_i P_i}{R_i + P_i}$.\n",
        "\n",
        "To test how the model generalizes to the training set, we hold back some of the training data by splitting the training data into a _training set_ and a _testing set_. We hold back 20% and stratify based on the data labels $y$, so each of the row counts in the confusion matrix should be $0.2 * 50 = 10$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a35f216f",
      "metadata": {
        "tags": [
          "split"
        ]
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, stratify=y)\n",
        "knn1.fit(Xtrain, ytrain)\n",
        "ypred1s = knn1.predict(Xtest)\n",
        "print(accuracy_score(ytest, ypred1s))\n",
        "print(confusion_matrix(ytest, ypred1s))\n",
        "print(classification_report(ytest, ypred1s, digits=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "377d5e94",
      "metadata": {},
      "source": [
        "Note the confusion (off-diagonal nonzero elements) between Iris species 2 and species 3. For comparison, we look at the confusion matrix when $k = 3$. Firstly, we try with all the training data (not holding any observations back for a test set)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9aebd1cd",
      "metadata": {
        "tags": [
          "kEquals3"
        ]
      },
      "outputs": [],
      "source": [
        "knn3 = KNeighborsClassifier(n_neighbors=3)\n",
        "knn3.fit(X, y)\n",
        "y_pred3 = knn3.predict(X)\n",
        "print(accuracy_score(y, y_pred3))\n",
        "print(confusion_matrix(y, y_pred3))\n",
        "print(classification_report(y, y_pred3, digits=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a49c6380",
      "metadata": {},
      "source": [
        "Note that 6 observations (3 each of species 2 and 3) are not classified the same as the human experts. However, this might also indicate something interesting about those observations. They could be outliers (not classified correctly) but, at the very least, they are extreme observations.\n",
        "\n",
        "Now we try holding back 20% of the training set for use as test observations, leaving 80% of the training data to train the classifier. We then look at what happens to the confusion matrix. Note that sampling the data like this could result in *better* relative performance, depending on what happens to the 6 problematic observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11b6fe4a",
      "metadata": {
        "tags": [
          "kEquals3metrics"
        ]
      },
      "outputs": [],
      "source": [
        "knn3.fit(Xtrain, ytrain)\n",
        "ypred3s = knn3.predict(Xtest)\n",
        "print(accuracy_score(ytest, ypred3s))\n",
        "print(confusion_matrix(ytest, ypred3s))\n",
        "print(classification_report(ytest, ypred3s, digits=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "094fb9a6",
      "metadata": {},
      "source": [
        "Now we try a Decision Tree Classifier from sklearn on the same Iris data. The same interface is used as the k-nearest-networks classifier.\n",
        "\n",
        "Again, we separate the data into training and test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbe4fbfa",
      "metadata": {
        "lines_to_next_cell": 2,
        "tags": [
          "fitDT"
        ]
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Derive Xtrain2, which is the \n",
        "XtrainDf = pd.DataFrame(data=Xtrain, columns=predNames)\n",
        "c1 = 'petal length (cm)'\n",
        "c2 = 'petal width (cm)'\n",
        "colNames = [c1, c2]\n",
        "Xtrain2 = XtrainDf[colNames]\n",
        "nTrain = Xtrain2.shape[0]\n",
        "\n",
        "XtestDf = pd.DataFrame(data=Xtest, columns=predNames)\n",
        "Xtest2 = XtestDf[colNames]\n",
        "Xcombined2 = pd.concat([Xtrain2, Xtest2])\n",
        "ycombined = np.hstack((ytrain, ytest))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "228a763f",
      "metadata": {},
      "source": [
        "We also look at comparing different decision trees to the `PetalWidth` $\\times$ `PetalLength` data, based on the following conditions\n",
        "\n",
        "1. maximum tree depth (2,3,4,5)\n",
        "2. choice of tree impurity algorithm (`gini` or `entropy`)\n",
        "\n",
        "which is 8 combinations in all."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bb87982",
      "metadata": {
        "tags": [
          "variedDT"
        ]
      },
      "outputs": [],
      "source": [
        "c1 = pattern.sub(\"\",c1.title()) # Make titlecase, then remove non-alphanumeric characters\n",
        "c2 = pattern.sub(\"\",c2.title())\n",
        "\n",
        "for treeDepth in range(2,6):\n",
        "  for criterion in [\"gini\",\"entropy\"]:\n",
        "    tree = DecisionTreeClassifier(criterion=criterion, max_depth=treeDepth, random_state=0)\n",
        "    tree2 = DecisionTreeClassifier(criterion=criterion, max_depth=treeDepth, random_state=0)\n",
        "\n",
        "    tree2.fit(Xtrain2, ytrain)\n",
        "\n",
        "    plotTitle = \"depth = %i %s %s fit to the %s dataset\" % (treeDepth, criterion, \"tree\", \"Iris\")\n",
        "    fileTitle = picDir + \"/depth_%i_%s_%s_%s_%s_%s.pdf\" % (treeDepth, criterion, \"decisionTree\", \"Iris\", c1, c2)\n",
        "\n",
        "    print(\"Plotting \"+fileTitle)\n",
        "    plot_2d_class(Xcombined2, ycombined, nTrain, tree2, plotTitle, fileTitle, cmap_light, cmap_bold)\n",
        "\n",
        "    ytree2 = tree2.predict(Xtest2)\n",
        "    print(accuracy_score(ytest, ytree2))\n",
        "    print(confusion_matrix(ytest, ytree2))\n",
        "    print(classification_report(ytest, ytree2, digits=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2162522",
      "metadata": {},
      "source": [
        "Previous decision trees were based on just two predictors (`PetalWidth` and `PetalLength`) as this made visualisation easier. However, if we include all 4 predictors, we see that the fit can improve (score improves to 0.97 from 0.93)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a27669e",
      "metadata": {
        "tags": [
          "entropyDT"
        ]
      },
      "outputs": [],
      "source": [
        "criterion = \"entropy\"\n",
        "treeDepth = 5\n",
        "tree = DecisionTreeClassifier(criterion=criterion, max_depth=treeDepth, random_state=0)\n",
        "tree.fit(Xtrain, ytrain)\n",
        "y_treeTest = tree.predict(Xtest)\n",
        "print(accuracy_score(ytest, y_treeTest))\n",
        "print(confusion_matrix(ytest, y_treeTest))\n",
        "print(classification_report(ytest, y_treeTest, digits=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c83005ea",
      "metadata": {
        "tags": [
          "visualiseDT"
        ]
      },
      "source": [
        "One of the main advantages of decision trees is the fact that they provide easily interpreted models for prediction. Indeed, the rules encoded in the tree can help to understand how the predictors combine and contribute to explaining the classification. As such, decision trees are often described as _white box_, where other algorithms (in particular, neural networks) are best seen as _black box_.\n",
        "\n",
        "To aid interpretation, `scikit-learn` can output the model in a graph description language such as [dot](https://www.graphviz.org/pdf/dotguide.pdf) using the `export_graphviz` method. If you wish, you can export the `dot` file and process it using tools, both command line such as [dotty](https://www.graphviz.org/pdf/dottyguide.pdf) and more general tools such as those listed [here](https://en.wikipedia.org/wiki/Graphviz). However, it is probably more convenient to use a `dot` postprocessor (`pydotplus`) directly from within the notebook to create an object that can be displayed in the notebook, or saved to a file as below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1582d1cd",
      "metadata": {
        "lines_to_next_cell": 2,
        "tags": [
          "visualiseDTcode"
        ]
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "from IPython.display import display\n",
        "import pydotplus\n",
        "from sklearn.tree import export_graphviz\n",
        "        \n",
        "dot_data = export_graphviz(\n",
        "    tree, \n",
        "    out_file=None,\n",
        "    feature_names=predNames,  \n",
        "    # the parameters below are new in sklearn 0.18\n",
        "    class_names=['setosa', 'versicolor', 'virginica'],  \n",
        "    filled=True,\n",
        "    rounded=True)\n",
        "\n",
        "graph = pydotplus.graph_from_dot_data(dot_data)\n",
        "display(Image(graph.create_png()))\n",
        "graph.write_pdf(picDir+\"/tree.pdf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "502c835f",
      "metadata": {
        "tags": [
          "visualiseDTdiscussion"
        ]
      },
      "source": [
        "The nodes in the tree a coloured according to their function and impurity. The root is white, reflecting the fact that it has the highest impurity (mix of lables). The _setosa_ instances are releatively easily identified, because they share the condition that their `PetalWidth < 0.8cm`. The remaining 80 instances are then split on `PetalLength < 4.95cm`. While this improves the entropy (the child nodes have entropy 0.446 and 0.179), each of them requires further recursive splitiing. Also note that predictors such as `PetalLength` can reappear in further nodes, though the split is on a different value of the predictor (5.05cm instead of 4.95cm).\n",
        "\n",
        "The terminal nodes (leaves) contain 40, 38, 4, 1, 3, 1 and 33 nodes each. "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
