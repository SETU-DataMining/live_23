{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical B - Association Rule Mining - Groceries DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practical we will perform a market basket analysis of transactional data from a small grocery store (9835 transactions, 169 products)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_context(\"paper\")\n",
    "\n",
    "from itertools import combinations, groupby\n",
    "from collections import Counter\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data used here was adapted from the Groceries dataset in the Apriori R package, and has been cleaned and simplified already.\n",
    "\n",
    "Note:\n",
    " \n",
    " * In a large grocery store, hint Insacart, there is a huge variety of items. There might be five brands of milk, a dozen different types of laundry detergent, and three brands of coffee. \n",
    "\n",
    " * If we can assume that the retailer is not terribly concerned with finding rules that apply only to a specific brand of milk or detergent we could remove all brand names and merge products. This reduces\n",
    "the number of groceries to a more manageable size, using broad categories such as chicken, frozen meals, margarine, and soda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download remote source if local copy is not available\n",
    "import requests, os\n",
    "url = \"https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/groceries.csv\"\n",
    "dataFile = \"data/groceries.csv\"\n",
    "if not os.path.isfile(dataFile):\n",
    "    r = requests.get(url)\n",
    "    with open(dataFile, 'wb') as f:  \n",
    "        f.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the first 5 rows of groceries.csv\n",
    "for line in open(dataFile).read().split(\"\\n\")[:5]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These lines indicate five separate grocery store transactions. The first transaction\n",
    "included four items: citrus fruit, semi-finished bread, margarine, and ready soups.\n",
    "In comparison, the third transaction included only one item, whole milk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in csv and convert to list of lists\n",
    "# Note the [:-1] to drop the last empty line in the CSV file\n",
    "\n",
    "transactions = [line.split(',') for line in open(dataFile).read().split(\"\\n\")][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the first 5 transactions (to compare with first 5 lines of file)\n",
    "from pprint import pprint\n",
    "pprint(transactions[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transcode transactions into a Data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions,sparse=False)\n",
    "\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_count = sum(sum(te_ary)) / 1662115\n",
    "print (\"Number of transactions = {:,} \".format(len(transactions)))\n",
    "print (\"Number of products = {:,} \".format(len(te.columns_)))\n",
    "print (\"Number of (non-unique) items sold = {:,}\".format(te_ary.sum()))\n",
    "\n",
    "print (\"Sparseness of transaction database {:.3%}\".format(te_ary.sum()/te_ary.size))\n",
    "print (\"Average number of items per transaction = {:.4}\".format(te_ary.sum()/len(transactions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size of Transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum to get nubmer of True along each row (each transaction)\n",
    "a = df.apply(lambda row: sum(row), axis=1)\n",
    "a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate plot of count of the number of transactions of the same size\n",
    "a.value_counts().plot.bar()\n",
    "plt.title(\"Distribution of Tranasaction Size\")\n",
    "plt.xlabel(\"Number of items\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or just output data   \n",
    "print(a.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can generate a set of statistics about the size of transactions. \n",
    "a.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So from above tables and bar plot we see:\n",
    "\n",
    " * A total of 2,159 transactions contained only a single item, while one transaction had 32 items. \n",
    " * The first quartile and median purchase size are 2 and 3 items respectively, implying that 25 percent of transactions contained two or fewer items and about half contained more or less than three items. \n",
    " * The mean of 4.409 matches the value we calculated earlier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association Rule Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequent Itemset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori\n",
    "a = apriori(df, min_support=0.1,use_colnames=True).sort_values(by='support',ascending=False)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing item support â€“ item frequency plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [next(iter(n)) for n in a[\"itemsets\"]]\n",
    "a.plot(kind='bar', title =\"Support for most frequent products\")\n",
    "plt.xticks(range(len(names)), names, rotation=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = apriori(df, min_support=0.05, max_len=1, use_colnames=True) \\\n",
    "    .sort_values(by='support',ascending=False)\n",
    "    \n",
    "names = [next(iter(n)) for n in a[\"itemsets\"]]\n",
    "a.plot.barh(title =\"Support for most frequent products\")\n",
    "plt.yticks(range(len(names)), names)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the sparse matrix for the first $k$ transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 100\n",
    "sns.set_style(\"white\")\n",
    "plt.figure(figsize=(10,5)) \n",
    "plt.imshow(1-te_ary[0:k], interpolation='none', cmap='gray')\n",
    "plt.xlabel(\"Items (Columns)\")\n",
    "plt.ylabel(\"Transactions (Rows)\")\n",
    "plt.title(\"Visualation of first %d transactions\" % k)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above diagram depicts a matrix with $k$ rows and 169 columns, indicating the $k$ transactions and 169 possible items we requested. Cells in the matrix are filled with black for transactions (rows) where the item (column) was purchased.\n",
    "\n",
    "A few columns seem fairly heavily populated, indicating some very popular items at the store, but overall, the distribution of dots seems fairly random. Given nothing else of note.\n",
    "\n",
    "This visualization can be a useful tool for exploring the data. For one, it may help with the identification of potential data issues. Columns that are filled all the way down could indicate items that are purchased in every transaction-a problem that could arise, perhaps, if a retailer's name or identification number was inadvertently included in the transaction datase.\n",
    "\n",
    "Additionally, patterns in the diagram may help reveal interesting segments of transactions or items, particularly if the data is sorted in interesting ways. For example, if the transactions are sorted by date, patterns in the black dots could reveal seasonal effects in the number or types of items people purchase. \n",
    "\n",
    "Perhaps around Christmas or Hanukkah, toys are more common; around Halloween, perhaps candy becomes popular. This type of visualization could be especially powerful if the items were also sorted into categories. In most cases, however, the plot will look fairly random, like static on an old CRT television screen which is not tuned to a channel. \n",
    "\n",
    "Keep in mind that this visualization will not be as useful for extremely large transaction databases because the cells will be too small to discern. Still, by combining it with sampling, you can view the sparse matrix for a randomly sampled set of transactions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets = apriori(df, min_support=0.1,use_colnames=True)\n",
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "frequent_itemsets = apriori(df, min_support=0.006,use_colnames=True)\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.8)\n",
    "rules.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we generated zero rules.\n",
    "\n",
    "If you think about it, this outcome should not have been terribly surprising. With the default support of 0.1, this means that in order to generate a rule, an item must have appeared in at least 0.1 * 9385 = 938.5 transactions. Since only eight items appeared this frequently in our data, it's no wonder we didn't find any rules.\n",
    "\n",
    "One way to approach the problem of setting support is to think about the minimum number of transactions you would need before you would consider a pattern interesting. For instance, you could argue that if an item is purchased twice a day (about 60 times) then it may be worth taking a look at. From there, it is possible to\n",
    "calculate the support level needed to find only rules matching at least that many transactions. Since 60 out of 9,835 equals 0.006, we'll try setting the support there first.\n",
    "\n",
    "Setting the minimum confidence involves a tricky balance. On one hand, if confidence is too low, then we might be overwhelmed with a large number of unreliable rulesâ€”such as dozens of rules indicating items commonly purchased with batteries. How would we know where to target our advertising budget then? \n",
    "\n",
    "On the other hand, if we set confidence too high, then we will be limited to rules that are obvious or inevitableâ€”like the fact that a smoke detector is always purchased in combination with batteries. In this case, moving the smoke detectors closer to the batteries is unlikely to generate additional revenue, since the two items were already almost always purchased together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with a confidence threshold of 0.25, which means that in order to be included in the results, the rule has to be correct at least 25 percent of the time. This will eliminate the most unreliable rules while allowing some room for us to modify behavior with targeted promotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "frequent_itemsets = apriori(df, min_support=0.006,use_colnames=True)\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.25)\n",
    "print (\"Generated {:,} rules\".format(len(rules)))\n",
    "rules.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get stats on objective measures\n",
    "rules[[\"support\",\"confidence\", \"lift\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we see summary statistics for the rule quality measures: support, confidence, and lift. Support and confidence should not be very surprising, since we used these as selection criteria for the rules. However, we might be alarmed if most or all of the rules were very near the minimum thresholdsâ€”not the case here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order rules by lift\n",
    "rules.sort_values(by='lift',ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Take the result of learning association rules and divide them into three categories:\n",
    "\n",
    " * Actionable\n",
    " * Trivial\n",
    " * Inexplicable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in rules.columns: \n",
    "    print(col) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking subsets of association rules (by rule length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new columnn storing the rule length\n",
    "rules[\"rule_len\"] = rules.apply(lambda row: len(row[\"antecedants\"])+len(row[\"consequents\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get stats on rules groupt by rule length\n",
    "rules[[\"rule_len\",\"support\", \"lift\"]].groupby(\"rule_len\").agg(['mean', 'count']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restrict analysis to rules of length 4 and order rules by lift\n",
    "rules[rules[\"rule_len\"]==4].sort_values(by='lift',ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Taking subsets of association rules (by content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restrict analysis to transactions involving berries\n",
    "df2 = df[df[\"berries\"]==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets = apriori(df2, min_support=0.006,use_colnames=True)\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.25)\n",
    "rules.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr = rules.apply(lambda row: \"berries\" in row[\"antecedants\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = apriori(df, min_support=0.003,use_colnames=True)\n",
    "a[\"itemset_len\"] = a.apply(lambda row: len(row[\"itemsets\"]), axis=1)\n",
    "a.groupby(\"itemset_len\").size().reset_index(name=\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# effect of minsupport on distribution of frequent itemsets\n",
    "sns.set_style(\"darkgrid\")\n",
    "for minsupport in [0.06, 0.006, 0.003, 0.002, 0.001]:\n",
    "    a = apriori(df, min_support=minsupport,use_colnames=True)\n",
    "    a[\"itemset_len\"] = a.apply(lambda row: len(row[\"itemsets\"]), axis=1)\n",
    "    d = a.groupby(\"itemset_len\").size().reset_index(name=\"count\")\n",
    "    d[\"count\"].plot.line(label=\"minsupp = %s\" % 0.01)\n",
    "plt.xticks(range(7))\n",
    "plt.legend()\n",
    "plt.xlabel(\"Size of itemset\")\n",
    "plt.ylabel(\"Number of Frequent itemsets\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
