{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9bdfb7e1",
      "metadata": {},
      "source": [
        "# Optical Character Recognition for MNIST handwritten characters"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "505e5dc2",
      "metadata": {},
      "source": [
        "OCR (Optical Character Recognition) is another classification problem. In this example, we wish to recognise hand-written digits from the famous NIST dataset, each of which is presented as an $8\\times 8$ array of pixel intensities.\n",
        "\n",
        "We are just going to focus on the problem of classifying each rasterised digit scan, and not on the other steps which include tokenising text, basic data cleaning etc.\n",
        "\n",
        "scikit-learn includes a built-in set of pre-formatted digits which we can use. The set is actually the MNIST (Modified NIST) set, but is functionally equivalent to the original NIST set, in relation to its classification challenges.\n",
        "\n",
        "## Looking at the data\n",
        "\n",
        "The data is included in `scikit-learn` and so can be loaded easily."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f2ef395",
      "metadata": {
        "tags": [
          "load"
        ]
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "digits = datasets.load_digits()\n",
        "digits.images.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09b69917",
      "metadata": {},
      "source": [
        "As usual, we review the training data before doing anything else. In this case, the data takes the form of rasterised images, so it makes sense to display them as such, overlaying each image with the label it was assigned by a human."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "395973de",
      "metadata": {
        "tags": [
          "firstLook"
        ]
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(10, 10, figsize=(8, 8))\n",
        "fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
        "\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(digits.images[i], cmap='binary', interpolation='nearest')\n",
        "    ax.text(0.05, 0.05, str(digits.target[i]),\n",
        "            transform=ax.transAxes, color='green')\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a12e086",
      "metadata": {},
      "source": [
        "Here the data per digit is simply each pixel value within an 8x8 grid. The example grid below represents a zero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f29805bd",
      "metadata": {
        "tags": [
          "quickView"
        ]
      },
      "outputs": [],
      "source": [
        "# The images themselves\n",
        "print(digits.images.shape)\n",
        "print(digits.images[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4307ab52",
      "metadata": {},
      "source": [
        "While it is better to display each instance as an 8x8 grid, each instance needs to be flattened into a single row with 64 elements (columns), as below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "513c12d0",
      "metadata": {
        "tags": [
          "flattenedData"
        ]
      },
      "outputs": [],
      "source": [
        "# The flattened data that is used to train the model.\n",
        "print(digits.data.shape)\n",
        "print(digits.data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ea1ee31",
      "metadata": {},
      "source": [
        "There are some nice facilities to count the number of different digits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4316a981",
      "metadata": {
        "tags": [
          "targetLabel"
        ]
      },
      "outputs": [],
      "source": [
        "# The target label\n",
        "from collections import Counter # https://stackoverflow.com/a/2392948\n",
        "c = Counter(digits.target)\n",
        "\n",
        "print(c.items())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03bb6127",
      "metadata": {},
      "source": [
        "Summarising, the data has 1797 samples in 64 dimensions and 10 ($0,\\ldots,9$) levels. The number of instances per level varies from 174 to 183."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8de8836",
      "metadata": {},
      "source": [
        "## Classifying the digits using logistic regression\n",
        "\n",
        "Logistic regression is an extension of regression where a change of variable is used to map the continuous (numerical-valued) prediction into categorical values for classification purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31958361",
      "metadata": {
        "tags": [
          "trainTestSplit"
        ]
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "seed=2\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(digits.data, digits.target,\n",
        "                                                random_state=seed, stratify=digits.target)\n",
        "print(Xtrain.shape, Xtest.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97fadb9b",
      "metadata": {},
      "source": [
        "We use logistic regression with an $\\ell_2$-based regularisation penalty (recall Week 5's discussion of regularisation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbb8de2b",
      "metadata": {
        "tags": [
          "LRfit"
        ]
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Get and configure a LogisticRegression object, with an L2 regularisation penalty\n",
        "clf = LogisticRegression(penalty='l2', max_iter=7600)\n",
        "\n",
        "# Fit the training data\n",
        "clf.fit(Xtrain, ytrain)\n",
        "\n",
        "# Using the beta parameters that have just been learned and are in clf, predict (recognise) the test data\n",
        "ypred = clf.predict(Xtest)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a543bdd",
      "metadata": {},
      "source": [
        "We check the classification accuracy score and confusion matrix as we did for the Iris Data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61f9a290",
      "metadata": {
        "tags": [
          "LRmetrics"
        ]
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "print(accuracy_score(ytest, ypred))\n",
        "confusionMat = confusion_matrix(ytest, ypred)\n",
        "print(confusionMat)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "982d62af",
      "metadata": {},
      "source": [
        "As can be seen, the confusion matrix has several off-diagonal nonzero terms. Because there are 10 labels, the confusion matrix is slightly harder to visualise than the Iris data, which had just 3 labels. We can get a better sense of its layout by plotting it as an image. Because all the values are nonnegative, but there is a large difference in size from the smallest (0) to the largest (45) with most values being at each end of the range, the square root of the values maps better into the Blue colour space used in the plot below:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5796b7c1",
      "metadata": {},
      "source": [
        "Make sure the directory exists beforehand to store the generated plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54a20455",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "picDir = \"output/pics\"\n",
        "if not os.path.exists(picDir):\n",
        "    os.makedirs(picDir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be384004",
      "metadata": {
        "tags": [
          "LRconfusionMatrix"
        ]
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "plt.imshow(np.sqrt(confusionMat),cmap='Blues', interpolation='nearest')\n",
        "plt.grid(False)\n",
        "plt.ylabel('true')\n",
        "plt.xlabel('predicted');\n",
        "plt.savefig(picDir+\"/logreg_digits_l2_confusionMatrix.pdf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9be38df0",
      "metadata": {},
      "source": [
        "We might also take a look at some of the outputs along with their predicted labels. Matching labels are <font color='green'>green</font> (as before) and unmatched labels are <font color='red'>red</font>:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04fa7aa8",
      "metadata": {
        "tags": [
          "LRdigitsAccuracy"
        ]
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(10, 10, figsize=(8, 8))\n",
        "fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
        "\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(Xtest[i].reshape(8, 8), cmap='binary')\n",
        "    ax.text(0.05, 0.05, str(ypred[i]),\n",
        "            transform=ax.transAxes,\n",
        "            color='green' if (ytest[i] == ypred[i]) else 'red')\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "fig.savefig(picDir+\"/digitsAccuracyCheck.pdf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f4e2c5f",
      "metadata": {},
      "source": [
        "Where they do not match, it is arguable what the original writing was meant to represent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "501f0051",
      "metadata": {
        "tags": [
          "ensemble_bagging"
        ]
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "# Configure the bagging classifier\n",
        "n_estimators=50\n",
        "baggingClf = BaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
        "                        n_estimators=n_estimators, random_state=0).fit(Xtrain, ytrain)\n",
        "ypredBagging = clf.predict(Xtest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35a0c462",
      "metadata": {
        "tags": [
          "BaggingMmetrics"
        ]
      },
      "outputs": [],
      "source": [
        "print(accuracy_score(ytest, ypredBagging))\n",
        "confusionMatBagging = confusion_matrix(ytest, ypredBagging)\n",
        "print(confusionMatBagging)\n",
        "\n",
        "import seaborn as sns\n",
        "plt.figure(figsize=(10,7))\n",
        "sns.set(font_scale=1.4) # for label size\n",
        "sns.heatmap(confusionMatBagging, annot=True, annot_kws={\"size\": 16}) # font size\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01db018f",
      "metadata": {
        "tags": [
          "ensemble boosting"
        ]
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa1fcbc7",
      "metadata": {
        "tags": [
          "Exercise"
        ]
      },
      "source": [
        "## Exercise\n",
        "\n",
        "Use the bagging and boosting classifier above with the NIST handwriting data and compare the results with the logistic regression one above.\n",
        "\n",
        "1. Which algorithm, if any, seems to handle this data better? Give reasons for your answer.\n",
        "2. Using GridSearchCV - does your ranking change? What might this tell you?\n",
        "3. One way of identifying outliers is to see whether they are discordant when different techniques are used. Which scanned digits do you think are outliers?\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
